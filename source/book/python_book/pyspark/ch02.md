第二章、弹性分布式数据集
==============================================

弹性分布式数据集 RDD 是一组不可变的JVM 对象的分布集且是spark的核心

2.1、RDD 的内部运行方式
------------------------------------------------------------------

RDD并行操作

spark最大的优势是：每个转换并执行，从而大大提高速度

2.2、创建RDD
------------------------------------------------------------------

```python

#创建集合
data = sc.parallelize([
    ("amner",22),("alfred",23),("sky",24)
])

#引入外部文件：
data_from_file = sc.textFile("path/file.txt.gz",4)
#后面的4参数代表被划分的分区个数

```
spark可以从多个文件系统中读取：如ntfs、fat这类的本地文件系统

支持多种数据格式： 文本、parquet、JSON、Hive tables 以及使用JDBC驱动程序可以读取关系数据库中的数据。

并可以自动处理压缩数据集  如果上面的.gz

2.3、全局作用域和局部作用域
------------------------------------------------------------------

spark可以在两种模式下运行：本地和集群

本地运行spark代码时和前面使用的python没有什么不同，然后将相同的代码部署到集群变回可能呆滞大量的困扰

这就需要了解spark是怎么在集群上执行工作任务的。

在集群模式中，提交执行任务时，任务五被发送给了我驱动程序节点或者主节点 。 该驱动程序节点为任务创建DAG 并且决定哪一个执行者 节点将运行特定的任务

然后该驱动程序只是工作者执行他们的任务 并将结束时的结果返回给驱动程序 然后在这致亲爱按  驱动程序为每一个任务的终止做准备 驱动程序中有一组变量和方法 以便工作者在RDD上执行任务

2.4、转换
------------------------------------------------------------------
转换可以调整数据集。包括映射、筛选、连接、转换数据集中的值。

.map转换： 可以说经常会用到map转换

```
data = data_from_file_conv_map(lambda row: int(row[16]))
```

将row[16]中转换数字

使用更多列，必须打包成一个数组 或者列表字典：
```
#这里书籍有语法错误？ 少了一个括号
data = data_from_file_conv.map(lambda row : (row[16],int(row[16]): )

data.take(5)
```

.filter() 转换

该方法可以从数据集中悬着元素  该元素集合特定的标准 

比如统计人数： 

```
data = data_from_file_conv.filter(lambda row : row[16] == '2014' and row[21] =='0' )
data.count()
```

.flatMap()转换

flatMap方法和map方法的工作类似 但是flatMap返回一个扁平的结果。flatMap方法把每一行看做一个列表对待 然后将所有的记录简单的加入到一起 通过传递一个空列表可以丢弃格式不正确的记录

.distinct() 转换

该方法返回指定列中不同值的列表  如果你想知道你的数据集或者验证这个数据集   该方法费用有用，检查性别列表是否质包含了男性或者女性将验证我们是否准确解析了数据集

```
data = data_from_file_conv.map(lambda row :row[5].distinct())

data.collect()
```
首先我们只提包含性别的列 接着使用distinct 方法只选择列表中不同的值 最后使用collect 打印返回值

.sample()转换

sample方法返回数据集的随机样本  


.leftOuterJoin()转换

就像在SQL中一样  根据两个数据集都有的值来简介两个RDD 并返回左侧RDD记录

.repartition()转换

重新对收集进行分区  改变了数据集分区的数量  

2.5、操作
------------------------------------------------------------------

take方法： 返回单个数据的前N行

collect方法： 返回给驱动程序并提出警告

reduce方法： 使用指定的方法减少RDD中的元素

count方法： 统计元素

saveAsTextFile方法： 保存txt文件到磁盘

foreach方法： 用迭代的方式应用相同的函数  和map比较  foreach按照一个接一个的方式

2.6、小结
------------------------------------------------------------------

RDD 是spark的核心  这些无schema数据结构在spark中处理最基本的数据结构 

spark中的转换是惰性的  它们旨在操作被调用时应用 

scala和pythonRDD之间的区别是速度  py 的慢很多。


