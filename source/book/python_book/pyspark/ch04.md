第四章、准备数据建模
==============================================

4.1、检查重复数据、未观测数据和异常数据(离群值)
------------------------------------------------------------------

``.dropDuplicates(..)`` 方法移除重复的行

``df = df.dropDuplicates()``

移除ID以外的行：

```
df = df.dropDuplicates( subset = [c for c in df.columns if c != "id"] )
```
subset 参数指示 dropDuplicates 方法只能查找subset参数指定的列

在上面的示例中，我们移除了具有相同的除ID外字段的值


计算ID的总数和ID唯一个数 可以使用 ``.agg(..)`` 方法

```
import pysparks.sql.function as fn
df.agg(
    fn.count('id').alias('count'),
    fn.countDistinct('id').alias('distinct')
).show()
```

使用 ``.count()`` 和 ``.countDistinct()`` 计算DataFrame的行数和ID的唯一数

使用唯一ID： ``.monotonicallymontonically_increasing_id()`` 能生成唯一ID

缺失数据：
```
df_miss.rdd.map(
    lambda row: (row['id'],sum([c == None for c in row()]))
).collect()
```
可以排除None的值

检查每一列中缺失的观测数据的百分比：
```
df_miss.agg(*[
    (1-(fn.count(c) / fn.count('*'))).alias(c+ "_missing")
]).show()
```

移除某一列：
```
df_miss_no_income = df_miss.select([
    c for c in df_miss.columns if c != "income"
])
```

``dropna`` 移除观测数据

``fillna`` 填充观测数据

``离群值`` ： 异常数据(离群值) 指那些与样本其余部分的分布显著偏离的观测数据 。显著的定义各不相同

计算每个特征的上下截点： ``approxQuantile方法`` 

4.2、熟悉你的数据
------------------------------------------------------------------

收集数据的基本信息 ： 有多个非缺失的观测数据、列 的平均值和标准偏差   最大值   最小值

第一，转换为spark DataFrame：

```
import pyspark.sql.types as typ 
```

第二： 读取数据并使用filter方法删除标题行  

接下来以每个都好分割出行并且将每个元素转换成整形

基本就是前面的例子的集合

4.3、可视化
------------------------------------------------------------------

matplotlib 和 bokch


4.4、小结
------------------------------------------------------------------

本章研究如何通过识别处理具有缺失值、重复数据和异常数据的数据集  来清理和准备数据建模  

