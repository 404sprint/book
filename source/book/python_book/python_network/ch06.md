第六章：屏幕抓取和其他实用程序
==============================================

`6.1、简介「略」`


`6.2、使用谷歌地图API搜索公司地址`

```
# 使用谷歌地图 API 搜索公司地址

from pygeocoder import Geocoder

def search_business(business_name):
    results = Geocoder.geocode(business_name)
    
    for result in results:
        print result

if __name__ == '__main__':
    business_name =  "Argos Ltd, London" 
    print "Searching %s" % business_name
    search_business(business_name)  
```

`6.3、使用谷歌地图URL搜索地理坐标`

```
# 使用谷歌地图 URL 搜索地理坐标

import argparse
import os
import urllib
import xml.etree.ElementTree as ET
from xml.dom import minidom

# https://developers.google.com/maps/documentation/geocoding/get-api-key
GOOGLE_MAPS_API_KEY = 'YOUR_GOOGLE_MAPS_API_KEY'

def find_lat_long(city):
        """Find geographic coordinates"""
        # Encode query string into Google maps URL
        
        url = 'https://maps.googleapis.com/maps/api/geocode/xml?address=' + city + '&key=' + GOOGLE_MAPS_API_KEY
        print 'Query: %s' % (url)
    
        # Get XML location from Google maps
        xml_str = urllib.urlopen(url).read()
        xml_doc = minidom.parseString(xml_str)
    
        if xml_doc.getElementsByTagName('status')[0].firstChild.nodeValue != 'OK':
            print '\nGoogle cannot interpret the city.'
            return
        else:
            # 默认取第一个数据
            lat = xml_doc.getElementsByTagName('lat')[0].firstChild.nodeValue
            lng = xml_doc.getElementsByTagName('lng')[0].firstChild.nodeValue
            print "Latitude/Longitude: %s/%s\n" % (lat, lng)

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='City Geocode Search')
    parser.add_argument('--city', action="store", dest="city", required=True)
    given_args = parser.parse_args()
    
    print "Finding geographic coordinates of %s" % given_args.city
    find_lat_long(given_args.city)
```


`6.4、搜索维基百科的文章`

```
# 搜索维基百科中的文章

import argparse
import re
import urllib
import urllib2
import json
import sys

SEARCH_URL = 'http://%s.wikipedia.org/w/api.php?action=query&list=search&srsearch=%s&sroffset=%d&srlimit=%d&format=json'

class Wikipedia:
    def __init__(self, lang='en'):
        self.lang = lang

    def _get_content(self, url):
        print url
        request = urllib2.Request(url)
        request.add_header('User-Agent', 'Mozilla/20.0')
       
        try:
            result = urllib2.urlopen(request)
        except urllib2.HTTPError, e:
            print "HTTP Error:%s" % (e.reason)
        except Exception, e:
            print "Error occured: %s" % str(e)
        return result

    def search_content(self, query, page=1, limit=10):
        offset = (page - 1) * limit
        url = SEARCH_URL % (self.lang, urllib.quote_plus(query), offset, limit)
        json_str = self._get_content(url).read()
        data = json.loads(json_str)

        search = data['query']['search']
        if not search:
            return

        results = []
        for article in search:
            snippet = article['snippet']
            snippet = re.sub(r'(?m)<.*?>', '', snippet)
            snippet = re.sub(r'\s+', ' ', snippet)
            snippet = snippet.replace('&quot;', '"')
            snippet = snippet.replace(' . ', '. ')
            snippet = snippet.replace(' , ', ', ')
            snippet = snippet.strip()
            
            results.append({
                'title' : article['title'].strip(),
                'snippet' : snippet
            })
        print results
        return results

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Wikipedia search')
    parser.add_argument('--query', action="store", dest="query", required=True)
    given_args = parser.parse_args() 
    wikipedia = Wikipedia()
    search_term = given_args.query
    print "Searching Wikipedia for %s" % search_term 
    results = wikipedia.search_content(search_term)
    print "Listing %s search results..." % len(results)
    
    # Windows 因为终端字符集（字体）问题可能无法打印输出
    for result in results:
        print "==%s== \n \t%s" % (result['title'], result['snippet'])
    print "---- End of search results ----"
```

`6.5、使用谷歌搜索股价`

```

# 使用谷歌搜索股价

import argparse
import urllib
import re
from datetime import datetime

SEARCH_URL = 'http://finance.google.com/finance?q='

def get_quote(symbol):
    content = urllib.urlopen(SEARCH_URL + symbol).read()
    m = re.search('Pre-market:&nbsp;<span class=bld id=".*?">(.*?)</span>', content)
    if m:
        quote = m.group(1)
    else:
        quote = 'No quote available for: ' + symbol
    return quote

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Stock quote search')
    parser.add_argument('--symbol', action="store", dest="symbol", required=True)
    given_args = parser.parse_args() 
    print "Searching stock quote for symbol '%s'" % given_args.symbol 
    print "Stock quote for %s at %s: %s" % (given_args.symbol, datetime.today(), get_quote(given_args.symbol))
```


`6.6、搜索Github中的源代码仓库`

```
# 搜索 GitHub 中的源代码仓库

import argparse
import requests
import json

SEARCH_URL_BASE = 'https://api.github.com/repos'

def search_repository(author, repo, search_for='homepage'):
    url = "%s/%s/%s" % (SEARCH_URL_BASE, author, repo)
    print "Searching Repo URL: %s" % url
    result = requests.get(url)
    if result.ok:
        repo_info = json.loads(result.text or result.content)
        # print repo_info
        print "Github repository info for: %s" % repo
        result = "No result found!"
        keys = [] 
        for key, value in repo_info.iteritems():
            if search_for in key:
                result = value
        return result

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Github search')
    parser.add_argument('--author', action="store", dest="author", required=True)
    parser.add_argument('--repo', action="store", dest="repo", required=True)
    parser.add_argument('--search_for', action="store", dest="search_for", required=True)
    
    given_args = parser.parse_args() 
    result = search_repository(given_args.author, given_args.repo, given_args.search_for)
    if isinstance(result, dict):
        print "Got result for '%s'..." % (given_args.search_for)
        for key, value in result.iteritems():
            print "%s => %s" % (key, value)
    else:
        print "Got result for %s: %s" % (given_args.search_for, result)
```


`6.7、读取BBC的新闻订阅源`

```
# 读取 BBC 的新闻订阅源

from datetime import datetime
import feedparser

BBC_FEED_URL = 'http://feeds.bbci.co.uk/news/%s/rss.xml'

def read_news(feed_url):
    try:
        data = feedparser.parse(feed_url)
    except Exception, e:
        print "Got error: %s" % str(e)

    for entry in data.entries:
        # Windows 因为终端字符集（字体）问题可能无法打印输出
        print entry.title
        print entry.link
        print entry.description
        print ''
        
if __name__ == '__main__':
    print "==== Reading technology news feed from bbc.co.uk (%s)====" % datetime.today()
    print "Enter the type of news feed: "
    print "Available options are: world, uk, health, sci-tech, business, technology"
    type = raw_input("News feed type:")
    read_news(BBC_FEED_URL % type)
    print "==== End of BBC news feed ====="

```


`6.8、爬去网页中的连接`

```
# 爬取网页中的链接

import argparse
import sys
import httplib
import re

processed = []

def search_links(url, depth, search):
    # Process http links that are not processed yet
    url_is_processed = (url in processed)
    if (url.startswith("http://") and (not url_is_processed)):
        processed.append(url)
        url = host = url.replace("http://", "", 1)
        print 'url', url
        path = "/"

        urlparts = url.split("/")
        if (len(urlparts) > 1):
            host = urlparts[0]
            path = url.replace(host, "", 1)
            print host, path

        # Start crawing
        print "Crawling URL path: %s%s " % (host, path)
        conn = httplib.HTTPConnection(host)
        req = conn.request("GET", path)
        result = conn.getresponse()

        # Find the links
        contents = result.read()
        all_links = re.findall('href="(.*?)"', contents)

        if (search in contents):
            print "Found " + search + " at " + url

        print " ==> %s: processing %s links" % (str(depth), str(len(all_links)))
        for href in all_links:
            # Find relative urls
            if (href.startswith("/")):
                href = "http://" + host + href

            # Recurse links
            if (depth > 0):
                search_links(href, depth-1, search)
    else:
        print "Skipping link: %s ..." % url

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Webpage link crawler')
    parser.add_argument('--url', action="store", dest="url", required=True)
    parser.add_argument('--query', action="store", dest="query", required=True)
    parser.add_argument('--depth', action="store", dest="depth", default=2)
    
    given_args = parser.parse_args() 
    
    try:
        search_links(given_args.url, given_args.depth,given_args.query)
    except KeyboardInterrupt:
        print "Aborting search by user request."
```

